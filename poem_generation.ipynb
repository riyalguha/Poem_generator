{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlAaf/hcv4yEzXuACaGfg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyalguha/Poem_generator/blob/main/poem_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w3zUSOVMvAZA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF__CPP_MIN_LOG_LEVEL\"] = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sZAqqmrKvQyR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "JfZgQMdOvh2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",)"
      ],
      "metadata": {
        "id": "585mMPQPvbnw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file,\"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"The length of text is {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvgFs1EPvtCz",
        "outputId": "7931e397-3e65-4bb6-efbf-5bbc2010fd2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of text is 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3dpQ15wE-L",
        "outputId": "7b7c5843-fa9d-443f-9839-87d2685a4e2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFQkZuKpwOuZ",
        "outputId": "7ce7dceb-2053-41d9-f80e-7c7bc343773c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing The Text"
      ],
      "metadata": {
        "id": "bTYnAmAm0Hsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizing the text"
      ],
      "metadata": {
        "id": "lrYFCFkaw7QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnokK1BCwhNm",
        "outputId": "f7ff1e5b-5ac2-444b-d71f-b47d79d9c4e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "6Tbpx8YyxGjh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHjyK-nzxU9a",
        "outputId": "e45dbdfd-2969-4383-9ee4-0e3a75ddbdbf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids =  tf.keras.layers.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(),invert = True,mask_token = None\n",
        ")"
      ],
      "metadata": {
        "id": "GWTAI-ZSyKwf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)"
      ],
      "metadata": {
        "id": "IIZqN0oUy6h0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9-cMH-zA4G",
        "outputId": "746b1ff2-0d60-41b3-de79-8dd7cf88ef14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars,axis = -1).numpy()## Joins tensors row-wise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y49yYksFzBik",
        "outputId": "33d3a6bf-b574-4e36-ecca-2d8a10184f28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids),axis = -1)"
      ],
      "metadata": {
        "id": "PzN71SayzM9p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Task"
      ],
      "metadata": {
        "id": "Tv20oIDK0KhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text,input_encoding = \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LigFpouz56x",
        "outputId": "d2dc58a4-9135-430a-e155-e4e74a36153c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "TPhHbws61RrU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(18):\n",
        "  print(chars_from_ids(ids).numpy().decode(\"utf-8\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Usf0ZbD1qfN",
        "outputId": "5b10829f-1468-43b2-ba94-7bb6db476e9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n",
            "B\n",
            "e\n",
            "f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epochs = len(text) // (seq_length+1)"
      ],
      "metadata": {
        "id": "LbvvyPMH2AXr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1,drop_remainder = True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCfY-1pC3Da1",
        "outputId": "a71f450d-cb7a-4063-afa8-b9dbae3aafbe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbMudG7U3k-B",
        "outputId": "53d2d35f-79ec-43de-c536-79e44b458fbb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we need to divide each sequence into (input,label) pairs where input is the current character and label is the next character"
      ],
      "metadata": {
        "id": "4yeD1mFP4UqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "q3lsD2Kx3y3O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P21cCeI-4MG_",
        "outputId": "84e43a56-ade3-408f-cdf9-1450e9a57f88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "XaLQ9_vk4PKH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_exp,target_exp in dataset.take(1):\n",
        "  print(\"INPUT: \",text_from_ids(input_exp).numpy(),len(input_exp.numpy()))\n",
        "  print(\"TARGET: \",text_from_ids(target_exp).numpy(),len(target_exp.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOqif4-G4iRA",
        "outputId": "95c6e40d-8401-4cff-ed96-a0f71e9630a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou' 100\n",
            "TARGET:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ' 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Batches"
      ],
      "metadata": {
        "id": "MMot_Vqx5_rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size\n",
        "batch_size = 64\n",
        "\n",
        "buffer_size = 10000\n",
        "dataset = (\n",
        "    dataset.shuffle(buffer_size)\n",
        "    .batch(batch_size,drop_remainder = True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNwQlxb_5DGI",
        "outputId": "9f6c4434-2dc6-4c4e-d917-c5881156d75f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n"
      ],
      "metadata": {
        "id": "RAWBCKcZccll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "6F0ev7w-btG3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyModel(tf.keras.Model):\n",
        "#   def __init__(self,vocab_size,embedding_dim,rnn_units):\n",
        "#     super().__init__(self)\n",
        "#     self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)#generates embeddings vectors from the integer encoded vocabulary indices\n",
        "#     self.gru1 = tf.keras.layers.GRU(rnn_units,return_sequences = True, return_state = True)#the RNN layer\n",
        "#     self.gru2 = tf.keras.layers.GRU(rnn_units,return_sequences = True,return_state = True)\n",
        "#     self.dense = tf.keras.layers.Dense(vocab_size)#generates the prob for next char\n",
        "\n",
        "#   def call(self,inputs,states = None, return_state = False, training = False):\n",
        "#     x = self.embedding(inputs,training = training)\n",
        "#     if states is None:\n",
        "#       states1 = self.gru1.get_initial_state(x)\n",
        "#       states2 = self.gru2.get_initial_state(x)\n",
        "#     else\n",
        "#       states1,states2 = states\n",
        "#     x,states1 = self.gru1(x,initial_state = states1, training = training)\n",
        "#     x.states2 = self.gru2(x,initial_state = states2,training = training)\n",
        "#     x = self.dense(x,training = training)\n",
        "\n",
        "#     if return_state:\n",
        "#       return x,[states1,states2]\n",
        "#     else:\n",
        "#       return x"
      ],
      "metadata": {
        "id": "j0yCAxCfdGLS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "3vmfQdBUKFCO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "                embedding_dim = embedding_dim,\n",
        "                rnn_units = rnn_units\n",
        "                )"
      ],
      "metadata": {
        "id": "gvekVBNYiZZS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try The Model"
      ],
      "metadata": {
        "id": "fNy1XgkVRMWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape,\n",
        "        \"# (batch_size,sequence_length,vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCYLT7V-Q2e9",
        "outputId": "903c1a99-e177-4e8e-e23c-71b061d70590"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size,sequence_length,vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3DGDsSR6mz",
        "outputId": "83e12891-0224-41fd-bed0-8e402a2b4f47"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "      example_batch_predictions[0],num_samples = 1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis = -1).numpy()"
      ],
      "metadata": {
        "id": "w0S_jCp3SO58"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_ukehluA6T",
        "outputId": "0361079d-44e2-4991-b746-7e9ccb300e8c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 20, 56, 47, 25, 65, 38, 13, 19, 51, 18, 55, 26, 61, 17, 34, 34,\n",
              "       17, 20, 45, 63, 44,  5, 24, 10,  2, 45, 10, 25, 17, 35, 53, 13, 12,\n",
              "       12, 55, 34, 51,  8,  2, 31,  7, 53,  2,  5, 58, 62, 64, 61, 16,  5,\n",
              "       40, 16, 47, 52, 50,  5, 19, 26, 47, 13, 61, 37, 29, 45, 15, 33, 55,\n",
              "       24, 65, 28, 26, 26,  8, 54, 61, 50, 45,  6, 39, 54, 36, 17, 41, 46,\n",
              "       42, 17, 19, 56,  0, 32, 64, 32, 32,  7, 11, 25, 64, 52, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\",text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\",text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1gDmQysuJgH",
        "outputId": "8c9bab28-98b1-46be-a874-324208622cd1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " b\" speed, is gone.\\n\\nLEONTES:\\nHow! gone!\\n\\nServant:\\nIs dead.\\n\\nLEONTES:\\nApollo's angry; and the heavens t\"\n",
            "\n",
            "Next Char Predictions: \n",
            " b\"EGqhLzY?FlEpMvDUUDGfxe&K3 f3LDVn?;;pUl- R,n &swyvC&aChmk&FMh?vXPfBTpKzOMM-ovkf'ZoWDbgcDFq[UNK]SySS,:LymD\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train The Model\n"
      ],
      "metadata": {
        "id": "oAzDhX4BvRbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)"
      ],
      "metadata": {
        "id": "nGOFN4gou3xu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch,example_batch_predictions)\n",
        "print(\"Prediction Shape: \",\n",
        "      example_batch_predictions.shape,\n",
        "      \"# (batch_size,sequence_length,vocab_size)\")\n",
        "print(\"Mean Loss   :\",example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKXsNKScwFwS",
        "outputId": "18f00624-b9c0-4c55-f829-e958c9318887"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction Shape:  (64, 100, 66) # (batch_size,sequence_length,vocab_size)\n",
            "Mean Loss   : tf.Tensor(4.1892514, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptr0i3R1wnxD",
        "outputId": "b0f18346-6f76-448e-af51-99545b3ffe7b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.97339"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam',loss = loss)"
      ],
      "metadata": {
        "id": "PZoB1zQ8w2B7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Checkpoints"
      ],
      "metadata": {
        "id": "ehRfQHYHxUQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix, save_weights_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "GE3GApLoxR4j"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "_rQIRF9SyKqT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SufDJAliyRMV",
        "outputId": "ecc2b3b8-964b-4c02-d2d3-01d82ca65927"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 21s 66ms/step - loss: 2.7179\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.9850\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.7102\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.5497\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.4514\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3836\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3308\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.2865\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.2454\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.2060\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.1664\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.1252\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.0819\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0361\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 0.9872\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 12s 64ms/step - loss: 0.9365\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.8842\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.8316\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7789\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 13s 64ms/step - loss: 0.7321\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 0.6858\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.6457\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.6109\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.5806\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.5537\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.5313\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 13s 63ms/step - loss: 0.5154\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.4999\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.4863\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 12s 64ms/step - loss: 0.4754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self,model,chars_from_ids,ids_from_chars,temperature = 1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "    self.model = model\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated\n",
        "    skip_ids = self.ids_from_chars([\"UNK\"])[:,None] # ADDs a new axis to skip_ids\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values = [-float(\"inf\")]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        dense_shape = [len(ids_from_chars.get_vocabulary())]\n",
        "    )\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self,inputs,states = None):\n",
        "    input_chars = tf.strings.unicode_split(inputs,\"UTF-8\")\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs = input_ids,\n",
        "                                          states = states,\n",
        "                                          return_state = True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:,-1,:]\n",
        "    predicted_logits = predicted_logits / self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits,num_samples = 1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis = -1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zrrIR1NNyaxy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "WwBQzYRw3gog"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(\n",
        "      next_char,states = states\n",
        "  )\n",
        "  result.append(next_char)\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'),\"\\n\\n\"+\"_\"*80)\n",
        "print(\"\\nRun Time\" , end - start)\n"
      ],
      "metadata": {
        "id": "Yw6i4k1DAuY3",
        "outputId": "9bddedce-2688-4209-ba3f-d8e3585a3d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Soft! with my sword, I'll say anon.\n",
            "\n",
            "SICINIUS:\n",
            "We will bestreme good scoff? so thou!\n",
            "\n",
            "LUCIO:\n",
            "Do not break his native wrongs\n",
            "To speak it into so deal out, if you live,\n",
            "To bitter where she purish me\n",
            "For being a little with mildny.\n",
            "But now the Duke of York is set on victory!\n",
            "Is't not the truth make me wife of it?\n",
            "Who hath a stirring drum? For now are dump,\n",
            "Lest to the queen's allied: shall I go see?\n",
            "\n",
            "LARTIUS:\n",
            "Dishonourable thou, joy in the thing, once more\n",
            "To-notice is leason.\n",
            "Hark you, like a cordial party be your king;\n",
            "But O, the horr perfidment should speak for requital\n",
            "powers call not so.\n",
            "Orge is the world--\n",
            "The father 'scaped Hisood to seem to kisl\n",
            "Tybalt waked from your exeliash,\n",
            "That broke her brothers harded my son.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "To sun by great parknce! no fourteen young Dies for the\n",
            "parter'st of those power: but 'tis done, gentle; earlish'd\n",
            "Your dauly sea, and that you will hence\n",
            "and do the friend: how could he stayd aloof,\n",
            "To die, and go with me into men: advise\n",
            "Here for thi \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run Time 3.1293814182281494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "id": "G0uLN9zuCzkv",
        "outputId": "3b3140d6-d360-4784-9daa-4013793da8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nA thousand time I'ld bid my peace by my bearing against his hateful.\\n\\nKING RICHARD III:\\nWhat stayed you have not wander'd to me with you?\\nHew does hence, do not proud me king;\\nFor God's sake, Richard, will not hear these sad tisle,\\nNay doth the pale clean-pinch'd than how.\\nI would take from of this is the rackous elects;\\nThat I, Somerver no dearer to hold you;\\nWhat answer many suits, and sword,\\nBut that thou sog or beauty dames that warm'd,\\nThen mother, wretched months happier and her grace!\\n\\nGLOUCESTER:\\nHow do you that friar?\\n\\nKING RICHARD III:\\nWhat is it?\\n\\nGLOUCESTER:\\nAlas, alas! she's not so. King Richard like a dead!\\nA child servant serving and her reputation that\\nI should do, and towards us all arrived,\\nAnd Warwick be spoken of my bragglent next;\\nAnd therefore, if you gave me to thine, she beg his\\ndeath: there is no more remember and process.\\nThis, do you perset!\\n\\nROMEO:\\nAy, nurse; what he dies hour?\\n\\nMENENIUS:\\nI thought, divide, cool'd upon Corioli instruct he:\\nYet, for thou wil\"\n",
            " b\"ROMEO:\\nSlain I unkerbed the thorns Offort! It sawith's thirst,\\nWhose hand shall vault that had dispark'd;\\nNot pull themselves to dids. There is no toward heart!\\nO, 'tis I thank you, go leave up.\\nHell matter upon her; with that hateful day.\\n\\nKING RICHARD III:\\nA cholacu such lips dead, my lords dead--\\nStafford, the faby dear men lives by me,\\nI mountably to say no.\\n\\nGRUMIO:\\nI know you for them; who\\nDare diefellor of my sons.\\n\\nMIRANDA:\\nCome on, and be;\\nMe protest shriek in all thing that we do not;\\nSpeak all your queen? now, by my soul, I'll die:\\nAbout his scounded mother now thy knee.\\n\\nKING RICHARD III:\\nWhat doth he? Where's\\nA fool in too her bore the loath\\nTo live answer now the crown to scraking fair!\\nThou godst be mercient directer? If I could stif\\nWith honour, like to have a stirring here,\\nThe second causes of our coaccibusure\\nHath none of tombssood towards ourselves resign. Let this lang's fie!\\nWho knows that I am Lord Angelo?\\n\\nDUKE DINIUS:\\nWhy, thou art a villain! A screature to the Capit\"\n",
            " b\"ROMEO:\\nNo; there's for the man's point.\\n\\nFirst Keeper:\\nNo;\\nI would temper it; for I'll none of--\\nprimore, master, there remains: all seeing heaven\\nWhich rouses not thy conscience take all post-his reign.\\n\\nKING EDWARD IV:\\nSo Juliettly, I do beseech your ladyship\\nTo be revenged on Ripers. Now to London,\\nTo sup out into testimoniest: blessed me out,\\nAnd so, my sinking one a smult, as I swore upon your sweet.\\n\\nLADY GREY:\\nBe present at up?\\n\\nBENVOLIO:\\nI do belde it his son God son, Sir, they\\nare advering I; if nothing else\\nI have forgot with fear appearing: how is\\nhere? What say you, Signioy to the diad?\\n\\nBENVOLIO:\\nThe sight of both at ning then good, you must\\nnot bring you say he sees, you quickly-kind of virtuous\\nNot deserved of them trafform'd to hark.\\nThou seest and made it armed against his doing:\\nIn cord, so hold consul, shame you there,\\nTo aurtle Nukerix, to brive discords and\\ncontrooming for usurp'd not another\\nAnd what to shame to merry means?\\nI read still be your forces bear them close.\\n\"\n",
            " b\"ROMEO:\\nIf they do me wrock, I may entreat for;\\nI will prove so, sir, to my power.\\n\\nClown:\\nAy, sir; what you have done, unlook'd for him: there was not firm?\\nOur souls hath caught me from from hence to France.\\nAnd in the street Isale: and\\nthough their course, how comest thou ride,\\nDoth choose but newly before infirmity\\nWas in thy mind, whose durst not seemeth some with one fair day?\\n\\nPRINCE EDWARD:\\nGo, I beseech you, hear me, Is by your person; holding him.\\nMy tale pursues King Edward's death, but die.\\n\\nRUTLAND:\\nI never did object itself a man\\nThat makes her brother just;\\nHer dreaments of the tomb, thy face both royal you;\\nGiving him one another, how I pray you;\\nI look'd your eye from many sad.\\nThey hearing down again, and from nine. For that Kate-killed my bon\\nTake in the mind. Gray!\\n\\nCitizens:\\nKins, madam: there's no remedy.\\n\\nJULIET:\\nI say the truth, I had a babe her proud arrogance.\\nTell me they have displeased my vow.\\n\\nCLIFFORD:\\nThy face is likewise food aught I shall.\\nAfford, Alaco, thou\"\n",
            " b\"ROMEO:\\nIf they do me wrong, fair lord's kinsmen and these the earth\\nWhich his usuble die for that reigns:\\nOr shall we call not?\\n\\nCLIFFORD:\\nHad he dindedly cas you himself penitent?\\nIn thy argives as we fine entrance to the wall!\\n\\nCLAUDIO:\\nAnd makes him draw were give.\\n\\nDUCHESS OF YORK:\\nA horse! my kingdom may give you guard\\nWith such dissentious rumbing Aged can and\\nAs brash aboard a news so quickly\\nconducting one among myself and as 'twiss them, even speaks the rank and place,\\nAnd warriors from yourself: but follows as he\\npersuades, as an assurace; and\\nwhat to my foes required in thy life that I have gone,\\nWith violent hearts on worthy mothing\\nStill bring him to the truth o' the consuls.\\n\\nBRUTUS:\\nBy the Copitious day, good sister!\\n\\nPAULINA:\\nGo so, Pray you.\\n\\nAUTOLYCUS:\\nA footman, sweet sorrow where I malm conserves?\\n\\nGentleman:\\nAy, Curtis.\\n\\nCURTIS:\\nWhere is the jaw drink disdried looks! Widow Dido!\\n\\nJULIET:\\nI see this blood away in wars and rather\\nFor beauty; so I quake, lest thy gold:\\nMake\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.141124963760376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")"
      ],
      "metadata": {
        "id": "ZYbhVENXDiZ0",
        "outputId": "8929dbf0-956c-4d5e-fd50-394664411b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e850038c5b0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ti-AYq_lMFVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}